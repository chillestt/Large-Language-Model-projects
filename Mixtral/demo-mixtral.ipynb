{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"03638a96888649dd9dc625a623eb6c34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffa7a23d200843068267e4e21553e98d","IPY_MODEL_ad60a3884afe47a9ab9b4a70a461d49b","IPY_MODEL_2248396d844b4f159fd895fa100f5875"],"layout":"IPY_MODEL_073cc65432b44fe29eae1c91471ff970"}},"ffa7a23d200843068267e4e21553e98d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_137e376507144fccbabb12bf1268f699","placeholder":"​","style":"IPY_MODEL_1a6a03868cdc4fb286b4ec580da9a391","value":""}},"ad60a3884afe47a9ab9b4a70a461d49b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_108098be0bdc49e791fb784d487fe175","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77eb47b1a453410b860b91a01357e98f","value":0}},"2248396d844b4f159fd895fa100f5875":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddc0e14f4448489499a534c5cf2f5945","placeholder":"​","style":"IPY_MODEL_6e8a37d65bf64bbaac84230613164936","value":" 0/0 [00:00&lt;?, ?it/s]"}},"073cc65432b44fe29eae1c91471ff970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"137e376507144fccbabb12bf1268f699":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a6a03868cdc4fb286b4ec580da9a391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"108098be0bdc49e791fb784d487fe175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"77eb47b1a453410b860b91a01357e98f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddc0e14f4448489499a534c5cf2f5945":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8a37d65bf64bbaac84230613164936":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67dc7ff4d0fe4af796308b2c8355d150":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b41dad78195344a6a77b570b1783f7a0","IPY_MODEL_494bb8b09d1f4f2abbc111eb5e6da130","IPY_MODEL_243efd24b8994278a406c597039d0fc4"],"layout":"IPY_MODEL_c2ccdf34885d41a0ab3f9b40c775b25a"}},"b41dad78195344a6a77b570b1783f7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39efadb9ccb048deab07969016e7bd38","placeholder":"​","style":"IPY_MODEL_d36446b0a9534f41a9f21b865b0a08f4","value":"config.json: 100%"}},"494bb8b09d1f4f2abbc111eb5e6da130":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a458bdef43543d0b7b7edc2883e2dc1","max":720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f85bc3a52a540558999a1ff25a813b3","value":720}},"243efd24b8994278a406c597039d0fc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_325f9b174633405483cf60cff12d652b","placeholder":"​","style":"IPY_MODEL_e60bfa41b7454322a908e01bb9caed65","value":" 720/720 [00:00&lt;00:00, 20.2kB/s]"}},"c2ccdf34885d41a0ab3f9b40c775b25a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39efadb9ccb048deab07969016e7bd38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36446b0a9534f41a9f21b865b0a08f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a458bdef43543d0b7b7edc2883e2dc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f85bc3a52a540558999a1ff25a813b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"325f9b174633405483cf60cff12d652b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e60bfa41b7454322a908e01bb9caed65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b7b50c298de414b85cc622ef5660dfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1e82a41293d41d6aa8807d9d575e86b","IPY_MODEL_a19471c4ecce428bbebab2a90f535861","IPY_MODEL_8a2c815f41b8483692a79165ebeaf3f6"],"layout":"IPY_MODEL_9782a56e37e24892a78b282295ae5ac0"}},"d1e82a41293d41d6aa8807d9d575e86b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9be1146cefb9416cb875741640dca611","placeholder":"​","style":"IPY_MODEL_15bfcbb901574f0583b8af3733f58b7a","value":"Loading experts: 100%"}},"a19471c4ecce428bbebab2a90f535861":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0dba9bd6ee94ca1a0084eabeac0e1ca","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83bcda80b79a4e6eaf5b0f27bcbc0ee0","value":32}},"8a2c815f41b8483692a79165ebeaf3f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b792acc1fff049fca658f836557c8e6f","placeholder":"​","style":"IPY_MODEL_dcf2b97ed0b7415fa02f38c8bb0e3009","value":" 32/32 [01:36&lt;00:00,  3.03s/it]"}},"9782a56e37e24892a78b282295ae5ac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9be1146cefb9416cb875741640dca611":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bfcbb901574f0583b8af3733f58b7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0dba9bd6ee94ca1a0084eabeac0e1ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83bcda80b79a4e6eaf5b0f27bcbc0ee0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b792acc1fff049fca658f836557c8e6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcf2b97ed0b7415fa02f38c8bb0e3009":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mixtral in Colab\n\nWelcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n\nTo learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub.","metadata":{"id":"OW1moHJ1TdhO"}},{"cell_type":"markdown","source":"One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n\n\n<details>\n\n<summary>How to balance between RAM and GPU VRAM usage</summary>\n\nYou can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n\nNote that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n</details>","metadata":{"id":"2-dvAX_hKZT4"}},{"cell_type":"markdown","source":"## Install and import libraries","metadata":{"id":"Y8MhvkC7TKEL"}},{"cell_type":"code","source":"# fix numpy in colab\nimport numpy\nfrom IPython.display import clear_output\n\n# fix triton in colab\n!export LC_ALL=\"en_US.UTF-8\"\n!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n!ldconfig /usr/lib64-nvidia\n\n!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n!cd mixtral-offloading && pip install -q -r requirements.txt\n!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n\nclear_output()","metadata":{"id":"f7qY7ebqX7T7","execution":{"iopub.status.busy":"2024-01-27T04:15:47.703181Z","iopub.execute_input":"2024-01-27T04:15:47.703480Z","iopub.status.idle":"2024-01-27T04:19:41.583626Z","shell.execute_reply.started":"2024-01-27T04:15:47.703453Z","shell.execute_reply":"2024-01-27T04:19:41.582500Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\n\nsys.path.append(\"mixtral-offloading\")\nimport torch\nfrom torch.nn import functional as F\nfrom hqq.core.quantize import BaseQuantizeConfig\nfrom huggingface_hub import snapshot_download\nfrom IPython.display import clear_output\nfrom tqdm.auto import trange\nfrom transformers import AutoConfig, AutoTokenizer\nfrom transformers.utils import logging as hf_logging\n\nfrom src.build_model import OffloadConfig, QuantConfig, build_model","metadata":{"id":"GgpjnV7fV49W","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["03638a96888649dd9dc625a623eb6c34","ffa7a23d200843068267e4e21553e98d","ad60a3884afe47a9ab9b4a70a461d49b","2248396d844b4f159fd895fa100f5875","073cc65432b44fe29eae1c91471ff970","137e376507144fccbabb12bf1268f699","1a6a03868cdc4fb286b4ec580da9a391","108098be0bdc49e791fb784d487fe175","77eb47b1a453410b860b91a01357e98f","ddc0e14f4448489499a534c5cf2f5945","6e8a37d65bf64bbaac84230613164936"]},"outputId":"1c10ea85-61e5-4572-aac9-0f02c4d2cb30","execution":{"iopub.status.busy":"2024-01-27T04:19:41.585689Z","iopub.execute_input":"2024-01-27T04:19:41.586024Z","iopub.status.idle":"2024-01-27T04:19:51.829796Z","shell.execute_reply.started":"2024-01-27T04:19:41.585995Z","shell.execute_reply":"2024-01-27T04:19:51.828657Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b0524adedb47f9bff188355cc3f3ab"}},"metadata":{}},{"name":"stderr","text":"Process ForkProcess-2:\nProcess ForkProcess-3:\nProcess ForkProcess-1:\nProcess ForkProcess-4:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n    res = self._recv_bytes()\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n    buf = self._recv(4)\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Initialize model","metadata":{"id":"OkSYibHcTQsH"}},{"cell_type":"code","source":"MODEL_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\nquantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\nstate_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:19:51.831339Z","iopub.execute_input":"2024-01-27T04:19:51.831756Z","iopub.status.idle":"2024-01-27T04:19:51.836196Z","shell.execute_reply.started":"2024-01-27T04:19:51.831724Z","shell.execute_reply":"2024-01-27T04:19:51.835419Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(quantized_model_name)\ndevice = torch.device(\"cuda:0\")\noffload_per_layer=5\nnum_experts = config.num_local_experts","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:21:00.514097Z","iopub.execute_input":"2024-01-27T04:21:00.514480Z","iopub.status.idle":"2024-01-27T04:21:00.610739Z","shell.execute_reply.started":"2024-01-27T04:21:00.514449Z","shell.execute_reply":"2024-01-27T04:21:00.609900Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"offload_config = OffloadConfig(\n    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n    offload_size=config.num_hidden_layers * offload_per_layer,\n    buffer_size=4,\n    offload_per_layer=offload_per_layer\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:21:01.393340Z","iopub.execute_input":"2024-01-27T04:21:01.394156Z","iopub.status.idle":"2024-01-27T04:21:01.398626Z","shell.execute_reply.started":"2024-01-27T04:21:01.394123Z","shell.execute_reply":"2024-01-27T04:21:01.397653Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"attn_config = BaseQuantizeConfig(\n    nbits=4,\n    group_size=64,\n    quant_zero=True,\n    quant_scale=True\n)\nattn_config[\"scale_quant_params\"][\"group_size\"] = 256","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:21:02.979435Z","iopub.execute_input":"2024-01-27T04:21:02.980310Z","iopub.status.idle":"2024-01-27T04:21:02.985027Z","shell.execute_reply.started":"2024-01-27T04:21:02.980274Z","shell.execute_reply":"2024-01-27T04:21:02.983948Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ffn_config = BaseQuantizeConfig(\n    nbits=2,\n    group_size=16,\n    quant_zero=True,\n    quant_scale=True\n)\nquant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:22:11.084196Z","iopub.execute_input":"2024-01-27T04:22:11.084549Z","iopub.status.idle":"2024-01-27T04:22:11.089339Z","shell.execute_reply.started":"2024-01-27T04:22:11.084519Z","shell.execute_reply":"2024-01-27T04:22:11.088351Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = build_model(\n    device=device,\n    quant_config=quant_config,\n    offload_config = offload_config,\n    state_path=state_path\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T04:22:51.228596Z","iopub.execute_input":"2024-01-27T04:22:51.229333Z","iopub.status.idle":"2024-01-27T04:24:46.217650Z","shell.execute_reply.started":"2024-01-27T04:22:51.229299Z","shell.execute_reply":"2024-01-27T04:24:46.216709Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dfa2bdf3483440ba20e029bf4c746c1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528c6db75cd742e79b9b8991087b8842"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Run the model","metadata":{"id":"Z4hBFYtPTUzT"}},{"cell_type":"code","source":"from transformers import TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\npast_key_values = None\nsequence = None\n\nseq_len = 0\nwhile True:\n    print(\"User: \", end=\"\")\n    user_input = input()\n    if user_input == \"end\":\n        break\n    print(\"\\n\")\n    \n    user_entry = dict(role=\"user\", content=user_input)\n    input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n    \n    if past_key_values is None:\n        attention_mask = torch.ones_like(input_ids)\n    else:\n        seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n        attention_mask = torch.ones([1, seq_len-1], dtype=torch.int, device=device)\n    \n    print(\"Mixtral: \", end=\"\")\n    result = model.generate(\n        input_ids = input_ids,\n        attention_mask = attention_mask,\n        past_key_values=past_key_values,\n        streamer=streamer,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.9,\n        max_new_tokens=512,\n        pad_token_id=tokenizer.eos_token_id,\n        return_dict_in_generate=True,\n        output_hidden_states=True,\n    )\n    print(\"\\n\")\n\n    sequence = result[\"sequences\"]\n    past_key_values = result[\"past_key_values\"]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T05:40:33.922372Z","iopub.execute_input":"2024-01-27T05:40:33.922783Z","iopub.status.idle":"2024-01-27T06:22:36.364907Z","shell.execute_reply.started":"2024-01-27T05:40:33.922751Z","shell.execute_reply":"2024-01-27T06:22:36.363898Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"User: ","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" show me the most effective way to manage time for better academic success\n"},{"name":"stdout","text":"\n\nMixtral: Time management is a crucial skill for academic success. Here are some strategies that can help you make the most of your time and improve your academic performance:\n\n1. Create a schedule: Plan out your days, weeks, and even months in advance. Make a timetable that includes classes, study time, breaks, and extracurricular activities. Make sure to allocate time for each task and prioritize your most important tasks.\n2. Set clear goals: Set specific, measurable, achievable, relevant, and time-bound (SMART) goals for yourself. This will help you stay focused and motivated, and will enable you to track your progress.\n3. Use a planner: Use a planner to keep track of your assignments, deadlines, and appointments. This will help you stay organized and reduce the chances of forgetting important tasks.\n4. Prioritize your tasks: Prioritize your tasks based on their importance and urgency. Focus on completing your most important tasks first, and then move on to less important tasks.\n5. Avoid procrastination: Procrastination can be a major time waster. To avoid procrastination, break down large tasks into smaller, manageable parts, and set specific deadlines for each part.\n6. Take breaks: Taking regular breaks can help you stay focused and refreshed. Use techniques like the Pomodoro Technique, where you work for 25 minutes and then take a 5-minute break.\n7. Study smarter, not harder: Use active learning techniques, such as flashcards, summarizing, teaching, and self-testing, instead of passive learning techniques like reading and highlighting.\n8. Minimize distractions: Identify and eliminate distractions that prevent you from focusing on your studies. Turn off your phone, find a quiet place to study, and use productivity apps to block distracting websites.\n9. Seek help when needed: Don't be afraid to seek help if you're struggling with a particular topic or assignment. Consult your professors, teaching assistants, or classmates for assistance.\n10. Reflect and adjust: Regularly reflect on your time management habits and adjust your strategies as needed. Identify what works for you and what doesn't, and make changes accordingly.\n\n\nUser: ","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" how to personal branding\n"},{"name":"stdout","text":"\n\nMixtral: Personal branding is the process of creating a distinct and recognizable image and reputation for yourself, highlighting your strengths, values, and personality. Here are some steps to help you develop your personal brand:\n\n1. Define your brand: Start by defining who you are, what you stand for, and what makes you unique. Identify your strengths, skills, values, and passions. Consider what makes you different from others in your field and what you want to be known for.\n2. Know your audience: Understand who your target audience is and what they are looking for. Consider their needs, interests, and values. Tailor your personal brand to resonate with them and stand out from the competition.\n3. Establish a consistent image: Create a consistent visual and messaging style across all your online and offline platforms. Use the same color scheme, font, and language to create a cohesive and recognizable brand.\n4. Leverage social media: Use social media platforms to showcase your expertise, build your network, and engage with your audience. Share valuable content, comment on industry news, and participate in relevant groups and communities.\n5. Build a professional network: Connect with like-minded professionals and influencers in your field. Attend industry events, join professional associations, and collaborate on projects.\n6. Create a personal website: Consider creating a personal website to showcase your portfolio, resume, and blog. Use your website as a hub for all your online activities and a platform to express your unique voice and perspective.\n7. Offer value: Provide valuable insights, tips, and resources to your audience. Share your knowledge, expertise, and experience. Offer to help others and provide solutions to their problems.\n8. Maintain your brand: Regularly review and update your personal brand to ensure it remains relevant and authentic. Be consistent and persistent in your efforts to build and maintain your brand.\n9. Be authentic: Authenticity is key to building a strong personal brand. Be true to yourself, your values, and your vision. Don't try to be someone you're not or copy someone else's style.\n10. Measure and adjust: Regularly measure and adjust your personal branding strategies based on your goals and feedback from your audience. Use analytics tools to track your progress and adjust your approach as needed.\n\n\nUser: ","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" how to become a content creator\n"},{"name":"stdout","text":"\n\nMixtral: Content creation is a popular and in-demand career path, with many opportunities available for those who are passionate, skilled, and persistent. Here are some steps to help you become a content creator:\n\n1. Identify your niche: Choose a topic or theme that you are passionate about and knowledgeable in. Consider your audience and what they are looking for. Narrow down your focus to a specific niche, where you can establish yourself as an expert and attract a loyal following.\n2. Develop your voice and style: Establish a unique and recognizable voice and style that reflects your personality, values, and expertise. Develop your own style, tone, and approach to content creation.\n3. Build your skills: Learn the basics of writing, editing, and visual storytelling. Familiarize yourself with the tools, platforms, and techniques used in content creation. Practice and refine your skills through regular writing, photography, or videography.\n4. Create a portfolio: Build a portfolio of your work that showcases your skills, style, and expertise. Include a mix of different formats, such as blog posts, videos, podcasts, and infographics.\n5. Establish your online presence: Create a website, blog, or social media profiles that feature your portfolio, bio, and contact information. Choose platforms that are relevant to your niche and audience. Engage with your followers and other content creators in your field.\n6. Build your audience: Attract and engage with your audience by sharing valuable and relevant content. Use social media, email marketing, and other channels to promote your work and reach a wider audience.\n7. Monetize your content: Explore various ways to monetize your content, such as sponsored posts, affiliate marketing, advertising, and selling digital products. Consider partnering with brands or other content creators to create sponsored content or collaborations.\n8. Maintain your brand: Regularly review and update your content creation strategies to ensure they align with your goals and audience needs. Stay true to your brand and values, and continue to deliver high-quality and engaging content.\n9. Stay up-to-date: Continuously learn and stay up-to-date with the latest trends, tools, and best practices in content creation. Follow industry leaders, attend conferences, and participate in online communities to stay informed and connected.\n10. Persevere: Building a successful career as a content creator\n\n\nUser: ","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" end\n"}]},{"cell_type":"code","source":"  user_entry = dict(role=\"user\", content=user_input)\n  input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n\n  if past_key_values is None:\n    attention_mask = torch.ones_like(input_ids)\n  else:\n    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n\n  print(\"Mixtral: \", end=\"\")\n  result = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    past_key_values=past_key_values,\n    streamer=streamer,\n    do_sample=True,\n    temperature=0.9,\n    top_p=0.9,\n    max_new_tokens=512,\n    pad_token_id=tokenizer.eos_token_id,\n    return_dict_in_generate=True,\n    output_hidden_states=True,\n  )\n  print(\"\\n\")\n\n  sequence = result[\"sequences\"]\n  past_key_values = result[\"past_key_values\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Zf4GkspecSm8","outputId":"a4ce58f9-30cc-414d-fa66-e5a28262d666"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"User: Write a funny poem about Python, please\n\n\n\n\n\nMixtral: There once was a language so bright,\n\nnamed Python, a helpful little wight.\n\nIt slithered through code with ease,\n\nNo bug stood a chance, to say the least.\n\n\n\nIts syntax so clean, and easy on the eyes,\n\nProgrammers from far and wide would shout their surprise.\n\nBut unlike its namesake, it's not sneaky or sly,\n\nIt's open source, and free to the sky!\n\n\n\nWith libraries so vast, it's a data geek's dream,\n\nFrom AI to web scraping, it's the ultimate scheme.\n\nIt's the Swiss Army knife of coding, or perhaps a black belt,\n\nIn the world of programming, it's the ultimate feel.\n\n\n\nSo, whether you're a beginner or a seasoned coder,\n\nPython's the language that will make you feel higher.\n\nWith a community so welcoming and a syntax so neat,\n\nIt's no wonder that Python is simply hard to beat!\n\n\n\nSo here's to the snake, in programming so great,\n\nMay it continue to dominate, until a much later date.\n\nIn the world of technology, it's here to stay,\n\nLong live Python, programming the Python way!\n\n\n\n\n\nUser: Thank you, that's cool! Now write a funny poem about LLMs (large language models)\n\n\n\n\n\nMixtral: There once was a model, quite large you see,\n\nMade by AI, for all humanity.\n\nIt spoke in tongues, from Old English to French,\n\nBut sometimes its answers, would leave you in a lurch.\n\n\n\nIt could write a poem, a story or more,\n\nBut when asked for advice, it would often be a bore.\n\nIt could summarize a book, in just a few lines,\n\nBut when it comes to humour, it tends to malfunction and crack bad lines.\n\n\n\nIt could answer a question, with a lot of thought,\n\nBut sometimes it would get stuck, and the answer would be naught.\n\nIt could imitate a human, in speech and in text,\n\nBut when it comes to jokes, it tends to miss the context.\n\n\n\nIt's a Large Language Model, so vast and so wide,\n\nBut when it comes to humour, it's often left outside.\n\nIt can do a lot of things, that would amaze you for sure,\n\nBut please, don't take it seriously, it has no sense of humour!\n\n\n\nSo here's to LLMs, so advanced and so cool,\n\nMay they continue to learn, and not be a fool.\n\nEven if they struggle, to understand a good joke,\n\nThey are still an amazing feat, for us to admire and evoke!\n\n\n\n\n\nUser: Summarize your poem about Python in two sentences\n\n\n\n\n\nMixtral: Python is a powerful and versatile programming language with a vast community and libraries, making it a top choice for both beginners and experts. It's praised for its clean and easy-to-understand syntax, as well as its ability to handle complex tasks with simplicity and efficiency.\n\n\n\n\n\nUser: What is your favourite joke in your poem about Python?\n\n\n\n\n\nMixtral: My personal favorite joke from the Python poem would be:\n\n\n\n\"It's a Swiss Army knife of coding, or perhaps a black belt,\n\nIn the world of programming, it's the ultimate feel.\"\n\n\n\nThis joke highlights the versatility and capabilities of the Python language, as well as the efficiency and ease of use it provides, making it a powerful tool for programmers. It's a fun and light-hearted way of saying that Python is a versatile and powerful language that can help you accomplish a lot with minimal effort.\n\n\n\n\n\nUser: "}]}]}